{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc9ee7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial import KDTree\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "from typing import Tuple, List\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c89c1a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    COLAB_ENV = True\n",
    "except:\n",
    "    COLAB_ENV = False\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93870a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idw_interpolation(latitude, longitude, df_temp_without_gauge, kdtree, p=2):\n",
    "    row = [latitude, longitude]\n",
    "    distances, indices = kdtree.query(row, k=5)\n",
    "    weights = 1 / (distances + 1e-6) ** p\n",
    "    values = df_temp_without_gauge.iloc[indices]['rain_mm'].values\n",
    "    return (np.sum(weights * values) / np.sum(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "025b2fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_date(ref_date, df_total, crossvalidation_path):\n",
    "    df_temp = df_total[(df_total['datetime'] == ref_date)]\n",
    "    gauges_loop = list(df_temp['gauge_code'].unique())\n",
    "    result_list = []\n",
    "    for gauge_code in gauges_loop[:]:\n",
    "        df_result = pd.DataFrame(columns=['gauge_code','datetime', \"lat\", \"long\",  'interpolated_rain_mm', 'rain_mm'])\n",
    "\n",
    "        index = (df_temp[df_temp['gauge_code']==gauge_code]).index[0]\n",
    "        latitude = df_temp.loc[index, \"lat\"]\n",
    "        longitude = df_temp.loc[index, \"long\"]\n",
    "        ground_value = df_temp.loc[index, \"rain_mm\"]\n",
    "\n",
    "        df_temp_without_gauge = df_temp[df_temp['gauge_code'] != gauge_code]\n",
    "\n",
    "        locations = df_temp_without_gauge[['lat', 'long']].values\n",
    "        kdtree = KDTree(locations)        \n",
    "\n",
    "        interpolated_value = idw_interpolation(latitude, longitude, df_temp_without_gauge, kdtree)\n",
    "        del df_temp_without_gauge, locations, kdtree\n",
    "        \n",
    "        df_result.loc[len(df_result)] = [gauge_code, ref_date, latitude, longitude, interpolated_value, ground_value]\n",
    "        result_list.append(df_result)\n",
    "    del df_temp\n",
    "    \n",
    "    df_final_result = pd.concat(result_list, ignore_index=True).drop_duplicates(ignore_index=True).sort_values('datetime', ignore_index=True)\n",
    "    del result_list\n",
    "    output_path = os.path.join(crossvalidation_path, f\"{ref_date.date().strftime('%Y_%m_%d')}_crossvalidation.h5\")\n",
    "    print(output_path)\n",
    "    df_final_result.to_hdf(output_path\n",
    "                            , key = 'table_crossvalidation'\n",
    "                            , mode = 'w'\n",
    "                            , append = False\n",
    "                            , complevel = 9\n",
    "                            , encoding=\"utf-8\")\n",
    "    del df_final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5755e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_in_chunks(file_path: str, chunk_size: int = 11_000_000) -> pd.DataFrame:\n",
    "    \"\"\"Load HDF5 data in chunks with memory optimization\"\"\"\n",
    "    dfs = []\n",
    "    with pd.HDFStore(file_path, mode='r') as store:\n",
    "        total_rows = store.get_storer('table_data').nrows\n",
    "        print(f\"Rows in table_data: {total_rows:,}\")\n",
    "        \n",
    "        for chunk in store.select('table_data', chunksize=chunk_size):\n",
    "            dfs.append(chunk)\n",
    "            print(f\"Processed chunk {len(dfs)} (approx. {len(dfs)*chunk_size:,}/{total_rows:,} rows)\")\n",
    "    \n",
    "    return pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c05749b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_prepare_data(df_data: pd.DataFrame, df_info: pd.DataFrame, \n",
    "                          start_date: str, end_date: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Filter data by date range and prepare date lists\"\"\"\n",
    "    # Filter by date range\n",
    "    mask = (df_data['datetime'] >= start_date) & (df_data['datetime'] <= end_date)\n",
    "    df_data = df_data.loc[mask].sort_values('datetime', ignore_index=True)\n",
    "    \n",
    "    # Create date list dataframe\n",
    "    df_date_list = (df_data[['datetime']]\n",
    "                   .drop_duplicates()\n",
    "                   .assign(year=lambda x: x['datetime'].dt.year)\n",
    "                   .sort_values('datetime', ignore_index=True))\n",
    "    \n",
    "    return df_data, df_date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9ac0f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_year(year: int, df_year: pd.DataFrame, crossvalidation_path: str) -> None:\n",
    "    \"\"\"Process data for a single year\"\"\"\n",
    "    date_ls = df_year['datetime'].unique().tolist()\n",
    "    date_ls.sort()\n",
    "    \n",
    "    print(f'\\nProcessing year: {year}')\n",
    "    print(f'Number of dates: {len(date_ls):,}')\n",
    "    print(f'Number of rows: {len(df_year):,}')\n",
    "    \n",
    "    Parallel(n_jobs=-2)(delayed(process_date)(ref_date, df_year, crossvalidation_path) \n",
    "                       for ref_date in date_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cb4d8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(file_path: str, crossvalidation_path: str) -> None:\n",
    "    \"\"\"Main processing pipeline\"\"\"\n",
    "    # Constants\n",
    "    START_DATE = '1963-04-14'\n",
    "    END_DATE = '1963-04-14'\n",
    "    \n",
    "    # Suppress HDF5 warnings\n",
    "    warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "    \n",
    "    try:\n",
    "        # Load metadata\n",
    "        df_info = pd.read_hdf(file_path, key='table_info')\n",
    "        \n",
    "        # Load main data in chunks\n",
    "        print(\"\\nLoading data...\")\n",
    "        df_data = load_data_in_chunks(file_path)\n",
    "        \n",
    "        # Filter and prepare data\n",
    "        print(\"\\nFiltering data...\")\n",
    "        df_data, df_date_list = filter_and_prepare_data(df_data, df_info, START_DATE, END_DATE)\n",
    "        \n",
    "        # Process by year\n",
    "        print(\"\\nStarting parallel processing...\")\n",
    "        for year, year_group in df_date_list.groupby('year'):\n",
    "            df_year_data = df_data[df_data['datetime'].isin(year_group['datetime'])]\n",
    "            df_year_data = pd.merge(df_year_data, df_info, on='gauge_code', how='inner')\n",
    "            process_year(year, df_year_data, crossvalidation_path)\n",
    "            \n",
    "        print('\\n\\nAll done!')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError occurred: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c77aefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data...\n",
      "Rows in table_data: 106,295,715\n",
      "Processed chunk 1 (approx. 11,000,000/106,295,715 rows)\n",
      "Processed chunk 2 (approx. 22,000,000/106,295,715 rows)\n",
      "Processed chunk 3 (approx. 33,000,000/106,295,715 rows)\n",
      "Processed chunk 4 (approx. 44,000,000/106,295,715 rows)\n",
      "Processed chunk 5 (approx. 55,000,000/106,295,715 rows)\n",
      "Processed chunk 6 (approx. 66,000,000/106,295,715 rows)\n",
      "Processed chunk 7 (approx. 77,000,000/106,295,715 rows)\n",
      "Processed chunk 8 (approx. 88,000,000/106,295,715 rows)\n",
      "Processed chunk 9 (approx. 99,000,000/106,295,715 rows)\n",
      "Processed chunk 10 (approx. 110,000,000/106,295,715 rows)\n",
      "\n",
      "Filtering data...\n",
      "\n",
      "Starting parallel processing...\n",
      "\n",
      "Processing year: 1963\n",
      "Number of dates: 1\n",
      "Number of rows: 3,696\n",
      "\n",
      "\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Define paths\n",
    "    if COLAB_ENV:\n",
    "        BASE_PATH = '/content/drive/MyDrive/BRain-D/Scripts e Dados/1 - Organized data gauge/BRAZIL'\n",
    "    else:\n",
    "        BASE_PATH = './1 - Organized data gauge/BRAZIL'\n",
    "    \n",
    "    FILE_PATH = os.path.join(BASE_PATH, 'DATASETS/BRAZIL_DAILY_1961_2024_QC.h5')\n",
    "    CROSSVAL_PATH = os.path.join(BASE_PATH, 'CROSSVALIDATION')\n",
    "    \n",
    "    main(FILE_PATH, CROSSVAL_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
