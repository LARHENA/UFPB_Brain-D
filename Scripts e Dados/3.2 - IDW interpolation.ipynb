{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6869502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # pandas is a powerful data manipulation library\n",
    "import numpy as np  # NumPy is the fundamental package for scientific computing with Python\n",
    "from scipy.spatial import cKDTree  # Replace KDTree with cKDTree for faster queries\n",
    "import xarray as xr # xarray is a powerful data structure that simplifies working with multi-dimensional arrays\n",
    "# import h5py # h5py is a common package for working with HDF5 files\n",
    "from statistics import mean\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2f7c26b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72151</th>\n",
       "      <td>-7.65</td>\n",
       "      <td>-74.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72153</th>\n",
       "      <td>-7.55</td>\n",
       "      <td>-74.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72150</th>\n",
       "      <td>-7.45</td>\n",
       "      <td>-74.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72159</th>\n",
       "      <td>-7.35</td>\n",
       "      <td>-74.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72157</th>\n",
       "      <td>-7.25</td>\n",
       "      <td>-74.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72155</th>\n",
       "      <td>-7.35</td>\n",
       "      <td>-34.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72161</th>\n",
       "      <td>-7.25</td>\n",
       "      <td>-34.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72162</th>\n",
       "      <td>-7.15</td>\n",
       "      <td>-34.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72166</th>\n",
       "      <td>-7.05</td>\n",
       "      <td>-34.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72168</th>\n",
       "      <td>-6.95</td>\n",
       "      <td>-34.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73111 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        lat   long\n",
       "72151 -7.65 -74.05\n",
       "72153 -7.55 -74.05\n",
       "72150 -7.45 -74.05\n",
       "72159 -7.35 -74.05\n",
       "72157 -7.25 -74.05\n",
       "...     ...    ...\n",
       "72155 -7.35 -34.75\n",
       "72161 -7.25 -34.75\n",
       "72162 -7.15 -34.75\n",
       "72166 -7.05 -34.75\n",
       "72168 -6.95 -34.75\n",
       "\n",
       "[73111 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_dataset_path = './1 - Organized data gauge/BRAZIL/DATASETS/BRAZIL_DAILY_1961_2024_QC.h5'\n",
    "\n",
    "table_grid = pd.read_hdf(cleaned_dataset_path, key='table_grid')\n",
    "table_grid.drop_duplicates().sort_values(['long', 'lat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eaa5b147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for IDW interpolation\n",
    "\n",
    "# In the IDW methodology, each of the nearest stations\n",
    "# selected for the interpolation at a query point is weighted\n",
    "# (Wk) by Wk=d(k)−p, where d is the distance of station k and\n",
    "# the specified query point. The p values is the power\n",
    "# parameter that we use p = 2, as suggested by \n",
    "# Dirks et al. (1998), Goovaert (2000), Lloyd (2005), Ly et al. (2011).\n",
    "\n",
    "def idw_interpolation(row, p=2, df_temp= pd.DataFrame(), grid_points=[]):\n",
    "    # Build KDTree from station locations\n",
    "    locations = df_temp[['lat', 'long']].values\n",
    "    kdtree = cKDTree(locations)\n",
    "    \n",
    "    # Find the indices and distances of the 5 nearest stations\n",
    "    spatial_resolution = 0.1 \n",
    "    step_size = spatial_resolution / 4\n",
    "\n",
    "    start_lat = row['lat'] - (spatial_resolution / 2)\n",
    "    end_lat = row['lat'] + (spatial_resolution / 2) + step_size  # Add step_size to include the endpoint\n",
    "    generated_latitudes = [round(start_lat + i * step_size, 6) for i in range(int((end_lat - start_lat) / step_size))]\n",
    "\n",
    "    start_lon = row['long'] - (spatial_resolution / 2)\n",
    "    end_lon = row['long'] + (spatial_resolution / 2) + step_size  # Add step_size to include the endpoint\n",
    "    generated_longitudes = [round(start_lon + i * step_size, 6) for i in range(int((end_lon - start_lon) / step_size))]\n",
    "\n",
    "    interpolated_value_avg = []\n",
    "\n",
    "    for lat in generated_latitudes:\n",
    "        for lon in generated_longitudes:\n",
    "            lat = round(lat, 2)\n",
    "            lon = round(lon, 2)\n",
    "            # Find the indices and distances of the 5 nearest stations\n",
    "            distances, indices = kdtree.query([lat, lon], k=5)\n",
    "            max_distance = 0\n",
    "            if max(distances) >= max_distance:\n",
    "                max_distance = max(distances)\n",
    "            # Compute the inverse distance weights\n",
    "            weights = 1 / (distances + 1e-6) ** p  # Adding a small value to prevent division by zero\n",
    "    \n",
    "            # Get the values at the nearest stations\n",
    "            values = df_temp.iloc[indices]['rain_mm'].values\n",
    "    \n",
    "            # Calculate the weighted average\n",
    "            interpolated_value = np.sum(weights * values) / np.sum(weights)\n",
    "            interpolated_value_avg.append(interpolated_value)\n",
    "    # print(\"max distance\", max_distance)\n",
    "            \n",
    "    interpolated_value_final = mean(interpolated_value_avg)\n",
    "    return interpolated_value_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49f8d39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a single date\n",
    "def process_date(ref_date, df_data_info, df_coords_temp, grid_points):\n",
    "    \"\"\"\n",
    "    Process a single date for IDW interpolation and save to NetCDF.\n",
    "    \"\"\"\n",
    "    # Filter stations with data for the current date\n",
    "    df_temp = df_data_info[df_data_info['datetime'] == ref_date]\n",
    "    \n",
    "    if df_temp.empty:\n",
    "        # print(f\"No data for {ref_date}. Skipping.\")\n",
    "        return\n",
    "    \n",
    "    # IDW interpolation\n",
    "    interpolated_rain = df_coords_temp.apply(lambda row: idw_interpolation(row, p=2, df_temp=df_temp, grid_points=grid_points), axis=1)\n",
    "    \n",
    "    # Create output DataFrame\n",
    "    df_precip = df_coords_temp.copy()\n",
    "    df_precip['rain_mm'] = interpolated_rain\n",
    "    df_precip['datetime'] = ref_date\n",
    "    \n",
    "    # Save to hdf5\n",
    "    output_path = f'./1 - Organized data gauge/BRAZIL/NetCDF/IDW_optimization_hdf/precipitation_idw_{ref_date.date()}.h5'\n",
    "    df_precip.to_hdf(output_path, key='table_data', mode='w', format='table', complevel=9, append=False)\n",
    "    print(f\"Saved: {output_path}\")\n",
    "\n",
    "    # # Save to NetCDF\n",
    "    # ds = xr.Dataset.from_dataframe(df_precip.set_index(['lat', 'long', 'datetime']))\n",
    "    # ds['rain_mm'].attrs['units'] = 'mm'\n",
    "    # output_path = f'./1 - Organized data gauge/BRAZIL/NetCDF/IDW_optimization/precipitation_idw_{ref_date.date()}.nc'\n",
    "    # ds.to_netcdf(output_path)\n",
    "    # print(f\"Saved: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0df41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    cleaned_dataset_path = './1 - Organized data gauge/BRAZIL/DATASETS/BRAZIL_DAILY_1961_2024_QC.h5'\n",
    "    df_data = pd.read_hdf(cleaned_dataset_path, key='table_data')\n",
    "    df_info = pd.read_hdf(cleaned_dataset_path, key='table_info')\n",
    "    df_coords = pd.read_hdf(cleaned_dataset_path, key='table_grid')\n",
    "    \n",
    "    # Merge data and info\n",
    "    df_data_info = pd.merge(df_data, df_info[['gauge_code', 'lat', 'long']], on='gauge_code', how='left')\n",
    "    del df_data, df_info\n",
    "    df_coords_temp = df_coords[['lat', 'long']]\n",
    "    \n",
    "    # Define start_date and end_date\n",
    "    start_date = '1961-01-01'\n",
    "    end_date = '2024-12-31'\n",
    "    \n",
    "    # Filter dates of interest\n",
    "    df_data_info = df_data_info.query(\"datetime >= @start_date and datetime <= @end_date\")\n",
    "    df_date_list = pd.DataFrame(df_data_info['datetime'].drop_duplicates().sort_values())\n",
    "    df_date_list = df_date_list.query(\"datetime >= @start_date and datetime <= @end_date\")\n",
    "    date_list = df_date_list['datetime'].tolist()\n",
    "    \n",
    "    # Precompute grid points\n",
    "    grid_points = df_coords_temp[['lat', 'long']].values\n",
    "    \n",
    "    # Parallel processing with joblib\n",
    "    Parallel(n_jobs=-2)(delayed(process_date)(ref_date, df_data_info, df_coords_temp, grid_points) for ref_date in date_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
