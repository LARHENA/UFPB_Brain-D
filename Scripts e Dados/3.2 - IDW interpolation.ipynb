{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6869502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # pandas is a powerful data manipulation library\n",
    "import numpy as np  # NumPy is the fundamental package for scientific computing with Python\n",
    "from scipy.spatial import cKDTree  # Replace KDTree with cKDTree for faster queries\n",
    "import xarray as xr # xarray is a powerful data structure that simplifies working with multi-dimensional arrays\n",
    "# import h5py # h5py is a common package for working with HDF5 files\n",
    "from statistics import mean\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2f7c26b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'No object named table_grid in the file'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m cleaned_dataset_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./1 - Organized data gauge/BRAZIL/DATASETS/BRAZIL_DAILY_1961_2024_QC.h5\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m table_grid \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_hdf(cleaned_dataset_path, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable_grid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m table_grid\u001b[38;5;241m.\u001b[39mdrop_duplicates()\u001b[38;5;241m.\u001b[39msort_values([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlong\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\pytables.py:452\u001b[0m, in \u001b[0;36mread_hdf\u001b[1;34m(path_or_buf, key, mode, errors, where, start, stop, columns, iterator, chunksize, **kwargs)\u001b[0m\n\u001b[0;32m    447\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    448\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey must be provided when HDF5 \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    449\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile contains multiple datasets.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    450\u001b[0m                 )\n\u001b[0;32m    451\u001b[0m         key \u001b[38;5;241m=\u001b[39m candidate_only_group\u001b[38;5;241m.\u001b[39m_v_pathname\n\u001b[1;32m--> 452\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m store\u001b[38;5;241m.\u001b[39mselect(\n\u001b[0;32m    453\u001b[0m         key,\n\u001b[0;32m    454\u001b[0m         where\u001b[38;5;241m=\u001b[39mwhere,\n\u001b[0;32m    455\u001b[0m         start\u001b[38;5;241m=\u001b[39mstart,\n\u001b[0;32m    456\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    457\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m    458\u001b[0m         iterator\u001b[38;5;241m=\u001b[39miterator,\n\u001b[0;32m    459\u001b[0m         chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m    460\u001b[0m         auto_close\u001b[38;5;241m=\u001b[39mauto_close,\n\u001b[0;32m    461\u001b[0m     )\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mLookupError\u001b[39;00m):\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, HDFStore):\n\u001b[0;32m    464\u001b[0m         \u001b[38;5;66;03m# if there is an error, close the store if we opened it.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\pytables.py:881\u001b[0m, in \u001b[0;36mHDFStore.select\u001b[1;34m(self, key, where, start, stop, columns, iterator, chunksize, auto_close)\u001b[0m\n\u001b[0;32m    879\u001b[0m group \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_node(key)\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 881\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo object named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in the file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    883\u001b[0m \u001b[38;5;66;03m# create the storer and axes\u001b[39;00m\n\u001b[0;32m    884\u001b[0m where \u001b[38;5;241m=\u001b[39m _ensure_term(where, scope_level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'No object named table_grid in the file'"
     ]
    }
   ],
   "source": [
    "cleaned_dataset_path = './1 - Organized data gauge/BRAZIL/DATASETS/BRAZIL_DAILY_1961_2024_QC.h5'\n",
    "\n",
    "table_grid = pd.read_hdf(cleaned_dataset_path, key='table_grid')\n",
    "table_grid.drop_duplicates().sort_values(['long', 'lat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa5b147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for IDW interpolation\n",
    "\n",
    "# In the IDW methodology, each of the nearest stations\n",
    "# selected for the interpolation at a query point is weighted\n",
    "# (Wk) by Wk=d(k)âˆ’p, where d is the distance of station k and\n",
    "# the specified query point. The p values is the power\n",
    "# parameter that we use p = 2, as suggested by \n",
    "# Dirks et al. (1998), Goovaert (2000), Lloyd (2005), Ly et al. (2011) and Xavier et al. (2016).\n",
    "\n",
    "def idw_interpolation(row, p=2, df_temp= pd.DataFrame(), grid_points=[]):\n",
    "    # Build KDTree from station locations\n",
    "    locations = df_temp[['lat', 'long']].values\n",
    "    kdtree = cKDTree(locations)\n",
    "    \n",
    "    # Find the indices and distances of the 5 nearest stations\n",
    "    spatial_resolution = 0.1 \n",
    "    step_size = spatial_resolution / 4\n",
    "\n",
    "    start_lat = row['lat'] - (spatial_resolution / 2)\n",
    "    end_lat = row['lat'] + (spatial_resolution / 2) + step_size  # Add step_size to include the endpoint\n",
    "    generated_latitudes = [round(start_lat + i * step_size, 6) for i in range(int((end_lat - start_lat) / step_size))]\n",
    "\n",
    "    start_lon = row['long'] - (spatial_resolution / 2)\n",
    "    end_lon = row['long'] + (spatial_resolution / 2) + step_size  # Add step_size to include the endpoint\n",
    "    generated_longitudes = [round(start_lon + i * step_size, 6) for i in range(int((end_lon - start_lon) / step_size))]\n",
    "\n",
    "    interpolated_value_avg = []\n",
    "\n",
    "    for lat in generated_latitudes:\n",
    "        for lon in generated_longitudes:\n",
    "            distances, indices = kdtree.query([lat, lon], k=5)\n",
    "            max_distance = 0\n",
    "            if max(distances) >= max_distance:\n",
    "                max_distance = max(distances)\n",
    "            # Compute the inverse distance weights\n",
    "            weights = 1 / (distances + 1e-6) ** p  # Adding a small value to prevent division by zero\n",
    "    \n",
    "            # Get the values at the nearest stations\n",
    "            values = df_temp.iloc[indices]['rain_mm'].values\n",
    "    \n",
    "            # Calculate the weighted average\n",
    "            interpolated_value = np.sum(weights * values) / np.sum(weights)\n",
    "            interpolated_value_avg.append(interpolated_value)\n",
    "    # print(\"max distance\", max_distance)\n",
    "            \n",
    "    interpolated_value_final = mean(interpolated_value_avg)\n",
    "    return interpolated_value_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f8d39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a single date\n",
    "def process_date(ref_date, df_data_info, df_coords_temp, grid_points):\n",
    "    \"\"\"\n",
    "    Process a single date for IDW interpolation and save to NetCDF.\n",
    "    \"\"\"\n",
    "    # Filter stations with data for the current date\n",
    "    df_temp = df_data_info[df_data_info['datetime'] == ref_date]\n",
    "    \n",
    "    if df_temp.empty:\n",
    "        # print(f\"No data for {ref_date}. Skipping.\")\n",
    "        return\n",
    "    \n",
    "    # IDW interpolation\n",
    "    interpolated_rain = df_coords_temp.apply(lambda row: idw_interpolation(row, p=2, df_temp=df_temp, grid_points=grid_points), axis=1)\n",
    "    \n",
    "    # Create output DataFrame\n",
    "    df_precip = df_coords_temp.copy()\n",
    "    df_precip['rain_mm'] = interpolated_rain\n",
    "    df_precip['datetime'] = ref_date\n",
    "    \n",
    "    # Save to hdf5\n",
    "    output_path = f'./1 - Organized data gauge/BRAZIL/NetCDF/IDW_optimization/precipitation_idw_{ref_date.date()}.h5'\n",
    "    df_precip.to_hdf(output_path, key='table_data', mode='w', format='table', complevel=9, append=False)\n",
    "    print(f\"Saved: {output_path}\")\n",
    "\n",
    "    # Save to NetCDF\n",
    "    ds = xr.Dataset.from_dataframe(df_precip.set_index(['lat', 'long', 'datetime']))\n",
    "    ds['rain_mm'].attrs['units'] = 'mm'\n",
    "    output_path = f'./1 - Organized data gauge/BRAZIL/NetCDF/IDW_optimization/precipitation_idw_{ref_date.date()}.nc'\n",
    "    ds.to_netcdf(output_path)\n",
    "    print(f\"Saved: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0df41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    cleaned_dataset_path = './1 - Organized data gauge/BRAZIL/DATASETS/BRAZIL_DAILY_1961_2024_QC.h5'\n",
    "    df_data = pd.read_hdf(cleaned_dataset_path, key='table_data')\n",
    "    df_info = pd.read_hdf(cleaned_dataset_path, key='table_info')\n",
    "    df_coords = pd.read_hdf(cleaned_dataset_path, key='table_grid')\n",
    "    \n",
    "    # Merge data and info\n",
    "    df_data_info = pd.merge(df_data, df_info[['gauge_code', 'lat', 'long']], on='gauge_code', how='left')\n",
    "    del df_data, df_info\n",
    "    df_coords_temp = df_coords[['lat', 'long']]\n",
    "    \n",
    "    # Define start_date and end_date\n",
    "    start_date = '2011-01-01'\n",
    "    end_date = '2011-01-01'\n",
    "    \n",
    "    # Filter dates of interest\n",
    "    df_data_info = df_data_info.query(\"datetime >= @start_date and datetime <= @end_date\")\n",
    "    df_date_list = pd.DataFrame(df_data_info['datetime'].drop_duplicates().sort_values())\n",
    "    df_date_list = df_date_list.query(\"datetime >= @start_date and datetime <= @end_date\")\n",
    "    date_list = df_date_list['datetime'].tolist()\n",
    "    \n",
    "    # Precompute grid points\n",
    "    grid_points = df_coords_temp[['lat', 'long']].values\n",
    "    \n",
    "    # Parallel processing with joblib\n",
    "    Parallel(n_jobs=-2)(delayed(process_date)(ref_date, df_data_info, df_coords_temp, grid_points) for ref_date in date_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
